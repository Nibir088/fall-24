
# Adversarial Robustness: Adversarial Defense

## Introduction

Recent work in deep learning has demonstrated the prevalence of adversarial examples, data points fed to a machine learning algorithm which are visually indistinguishable from “normal” examples, but which are specifically tuned so as to fool or mislead the machine learning system. In response, adversarial defenses have been an imperative object of study in machine learning, computer vision, natural language processing, and many other domains. In this presentation, we will discuss four different adversarial defense methods along with their performance under different white-box and black-box attacks.

## Motivations

Recent history in adversarial classification has followed something of a virtual “arms race”: practitioners alternatively design new ways of hardening classifiers against existing attacks, and then a new class of attacks is developed that can penetrate this defense. Somewhat memorably, many of the adversarial defense papers at ICLR 2018 conference were broken prior to the review period completing. On the other hand, recent breakthroughs in computer vision and natural language processing are bringing trained classifiers into the center of security-critical systems. Important examples include vision for autonomous cars, face recognition, and malware detection. These developments make security aspects of machine learning increasingly important, motivating practitioners to design increasingly robust training algorithms.

## Methods

### Distillation

Defensive distillation is a technique designed to safeguard deep neural networks (DNNs) against adversarial attacks. The process begins with the training of an initial neural network, known as the teacher model. This training is conducted in a somewhat unconventional manner, where the softmax layer's temperature is set unusually high. The rationale is that a high temperature in the softmax function leads to "softened" probability outputs instead of sharp, definitive predictions.

Once the teacher model is trained, it's used to infer the class probabilities of the training dataset. However, due to the elevated softmax temperature during training, these are not typical crisp probabilities. Instead, they are nuanced and softened, providing a rich and detailed representation of the model's confidence across various classes for each training sample. The next phase involves the training of a second network, referred to as the student model. The student model is trained using the same input data as the teacher, but instead of using the original hard labels, it learns from the softened probabilities generated by the teacher model. Crucially, the student model is also trained with the softmax temperature set high, mirroring the conditions under which the teacher's outputs were generated. The underlying theory is that training the student model to replicate the teacher's softened output allows it to inherit a more nuanced understanding of the data, improving its generalization capabilities and making it less sensitive to small, adversarial perturbations in its inputs. This is because the softened outputs encapsulate richer information about the data distribution and inter-class relationships than hard labels, guiding the student model to a more informed and robust parameter space.

Finally, when deploying the student model in a real-world setting or for inference purposes, the softmax temperature is normalized back to 1. This adjustment returns the model to producing sharp predictions suitable for actual use cases. Despite this reversion to standard temperature settings for deployment, the benefits of the training process persist. The student model, having learned in a high-temperature regime from softened outputs, demonstrates enhanced resilience against adversarial examples. It has internalized a more general and robust representation of the data, making it challenging for adversaries to exploit the model through the subtle perturbations characteristic of adversarial attacks. Thus, defensive distillation serves as a potent strategy to bolster the defenses of neural networks against the growing threat of adversarial manipulations.

### Robust Optimzation

Two key elements were focused on: a) train a sufficiently high capacity network, b) use the strongest possible adversary. The researchers use projected gradient descent(PGD) as the adversary of choice, starting from a random perturbation around the natural example. A wide range of adversaries were used to evaluate the trained models, this includes:

White-box attacks with PGD for a different number of iterations and restarts, denoted by Source A

White-box attacks with PGD using the Carlini-Wagner (CW) loss function (directly optimizing the difference between correct and incorrect logits). This is denoted as CW, where the corresponding attack with a high confidence parameter (κ = 50) is denoted as CW+.

Black-box attacks from an independently trained copy of the network, denoted A’.

Black-box attacks from a version of the same network trained only on natural examples, denoted 

Black-box attacks from a different convolution architecture, denoted B, described in Tramer et al. 2017 .

For the MNIST dataset, researchers run 40 iterations of projected gradient descent as adversary, with step size of 0.01, perturbation of size . The network consists of two convolutional layers with 32 and 64 filters respectively, each followed by a 2x2 max-pooling, and a fully connected layer of size 1024.

For the CIFAR10 dataset, the architecture the researcher uses is the original ResNet and its 10x wider variant. A network against a PGD adversary with projected gradient descent was trained again, this time using 7 steps of size 2, and a total . 

Another two additional experiments were run to perform a broader evaluation of the adversarial robustness of the model. Researcher examined the model’s resistance to ℓ∞-bounded attacks for varying ε values and to attacks bounded by ℓ2-norm instead of ℓ∞-norm.  For ℓ2-bounded PGD attacks, we moved in the gradient direction (not its sign) and normalized the steps to a fixed norm for easier step size tuning. All PGD attacks used 100 steps with a step size of 2.5 • ε /100 to ensure reaching the boundary of the ℓ-ball from any starting point within it (and still allow for movement on the boundary). The models were trained against ℓ∞-bounded attacks with the original ε value of .3 for MNIST and ε = 8 for CIFAR10.

### Provable defense via convex outer adversarial polytope
In the field of deep learning, particularly for networks using ReLU (Rectified Linear Unit) activation functions, it's crucial to understand their vulnerability to adversarial attacks. This is where the concept of the 'adversarial polytope' comes into play. It's a geometric representation that describes the potential perturbations that can mislead a neural network.
To evaluate the robustness of these networks, researchers formulated an optimization problem. Instead of solving it directly, which can be computationally intensive, they tackled its dual problem through a linear program. This approach simplifies the process by breaking it down into smaller sub-problems. They incrementally calculate the upper and lower bounds for each activation within the network, applying the dual perspective. By doing so, they obtained a scalable and efficient way to estimate the robustness of ReLU networks against adversarial examples.
During the paper, they mentioned the Explained outer bounds on the adversarial polytope for the first part of methodology. Firstly, they constructed a convex outer bound on this adversarial polytope. They mentioned that if no point within this outer approximation exists that will change the class prediction of an example, no point within a true adversarial polytope can change the prediction either. As shown in the figure below.

And they used the outer bound below on the adversarial polytope to replace the ReLU constraints. Furthermore, they also proved that the bound is robust and they consider the dual problem of the LP (Theorem 1).                   
        	        	
Then they brought a method to computing activation bounds, the results will be disclosed on the Key finding part.


### TRADES(TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization)

This study is motivated by the trade-off between natural and robust errors, which are defined below:

Natural (Classification) Error: 

Robust (Classification) Error: 

According to the theoretical bounds proven in the paper, in order to minimize the difference between the robust error of classifier f and the natural error of the best achievable classifier, we can minimize:

Algorithmically, the minimax problem is extended to the case of multi-class classifications by replacing φ with a multi-class calibrated loss L(·, ·). The pseudocode of adversarial training procedure, which aims at minimizing the empirical form of problem the minimax problem, is displayed below:



Note that the previous two algorithms for adversarial defense are based on robust optimization. Most results in this direction involve algorithms that approximately minimize:

In complex problem domains, however, this objective function might not be tight as an upper bound of the robust error, and may not capture the trade-off between natural and robust errors.


## Key Findings

### Distillation

The study presented in the paper evaluates the efficacy of defensive distillation as a countermeasure against adversarial attacks on deep neural networks (DNNs). Adversarial attacks involve subtly modified inputs designed to deceive DNNs into making incorrect predictions. The paper's results highlight the significant impact of defensive distillation in bolstering DNN resilience against such attacks.

In their experiments, the authors focused on two well-known datasets, MNIST and CIFAR10, and demonstrated that defensive distillation drastically reduced the success rate of adversarial sample crafting. Specifically, for the MNIST dataset, the success rate of adversarial attacks plummeted from 95.89% to a mere 0.45%, and for the CIFAR10 dataset, from 87.89% to 5.11%. This remarkable reduction underscores the effectiveness of defensive distillation in enhancing network security without detrimentally affecting classification accuracy. In fact, the study found that accuracy variations were minimal, staying below 1.37% even when distillation was applied.

Furthermore, the research provided insights into the mechanism behind the success of defensive distillation. It was shown to decrease the network's sensitivity to input perturbations, meaning that the DNNs became less prone to misclassification due to minor input changes. This was quantified by a reduction in the amplitude of adversarial gradients by substantial factors, implying that adversaries would need to introduce more pronounced, and therefore more detectable, changes to inputs to induce misclassifications. Additionally, defensive distillation significantly increased the robustness of the DNN models. In the context of the study, robustness refers to the network's ability to resist adversarial manipulation; specifically, it quantifies the minimum percentage of input features that need to be modified to deceive the network. Post-distillation, the MNIST-based model exhibited a 790% increase in robustness, and the CIFAR10-based model saw a 556% increase. This heightened robustness indicates that a much larger proportion of the input data would need to be altered to compromise the network, enhancing the model's utility in security-sensitive applications.

In summary, the study conclusively demonstrates that defensive distillation is a potent strategy for mitigating the risks posed by adversarial attacks on DNNs. It effectively enhances the resilience and robustness of neural networks, ensuring their reliability and performance in adversarial settings, all while maintaining high classification accuracy and introducing minimal computational overhead.


### Robust Optimization (Projected Gradient Descent Approach)

The researcher found that their models, when subjected to smaller ε values than those used during training, maintained or improved accuracy. However, a significant drop in robustness was observed for MNIST at slightly larger ε values. Despite this, the ℓ∞-trained model demonstrated greater resilience to ℓ2 attacks compared to a standard model. The researcher also noted that PGD was unable to find adversarial examples even for substantial ε values, suggesting that the models might not be as robust as initially thought. This was further supported by subsequent work which found that PGD tends to overestimate the ℓ2-robustness of these models. Despite these challenges, the researcher found that when attacked with a decision-based attack, which doesn’t rely on model gradients, the model was significantly more brittle against ℓ2-bounded attacks compared to a standard model. This led to the conclusion that learned threshold filters, which mask the gradient and prevent PGD from maximizing the loss, play a significant role in these observations.

### Provable defense via convex outer adversarial polytope
From the theoretical part, this paper also guarantees that the activation boundary is robust, there are many theorems proposed with detailed proof in Appendix.

In analyzing the results of the experiments conducted by the authors, they emphasize that in a single-layer neural network with 500 hidden units trained on the Human Activity Recognition (HAR) dataset, regardless of the size of the employed ε, the robustness bounds remain narrow after training, and the post-training bounds are in line with the error rates achievable with the Fast Gradient Symbolization Method (FGSM), which suggests that robustness bounds are an accurate indicator of the network's ability to defend against adversarial attacks accuracy metrics. This consistency is maintained for different values of ε, thus emphasizing the reliability of the robustness bounds as a measure of the strength of the network defense. As shown in the following Table.


What is more, after evaluating the loss curve from different datasets, there is no doubt that proposed method will bring the curve to have few fluctuations, which were the intuitive expression of the robustness. The Fig.12 above was an example for the performance of robustness.

### TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization)

The effectiveness of the proposed method is verified under both white-box and black-box threat models.

White-box attacks
The table below shows that the proposed defense method can significantly improve the robust accuracy of models, which is able to achieve robust accuracy as high as 56.61% on the CIFAR10 dataset.

Black-box attacks
The results of the black-box attacks are summarized in the two tables below. In both tables, two source models (noted in the parentheses) are used to generate adversarial perturbations: we compute the perturbation directions according to the gradients of the source models on the input images. It shows that our models are more robust against black-box attacks transferred from naturally trained models and robust optimization method’s models. Moreover, our models can generate stronger adversarial examples for black-box attacks compared with naturally trained models and robust optimization method’s models.



## Critical Analysis

### Distillation as a Defense

One significant limitation is that defensive distillation is inherently limited to DNN models that utilize an energy-based probability distribution, where the temperature parameter can be defined. This constraint is due to the reliance on the softmax function, which utilizes temperature to produce probability vectors. As such, extending defensive distillation to other machine learning models that do not fit this criterion would require considerable additional research. Additionally, while defensive distillation has shown promise in increasing the resilience of DNNs to adversarial attacks and improving their generalization capabilities outside their training set, it does not guarantee immunity against all forms of attacks. The method's effectiveness against different perturbation types and sophisticated attack methods remains to be thoroughly tested. Furthermore, despite empirical evidence showing improved robustness and reduced model sensitivity to input perturbations, the intrinsic non-convexity and non-linearity of DNN training can lead to suboptimal solutions, implying that there is no assurance the distilled model will always perform better than non-distilled counterparts in adversarial settings. This points to the need for ongoing research to address these challenges and refine the approach for broader applicability and more robust defense mechanisms against a wider array of adversarial tactics.

### Robust Optimization

The study exhibits several strengths, including its systematic exploration of adversarial robustness through robust optimization, which offers valuable insights into improving model resilience against attacks. The comprehensive evaluation against various adversaries and attack scenarios enhances the robustness assessment's reliability. However, weaknesses include potential oversights in considering real-world adversarial scenarios and limited generalizability beyond the chosen datasets and architectures. Biases may exist in the emphasis on adversarial examples as the primary metric, potentially overshadowing other important aspects of model performance. Ethical considerations arise concerning the implications of deploying potentially vulnerable models in critical applications and the broader societal impact of adversarial manipulation. Addressing these issues could strengthen the study's contributions and ensure more robust and ethically sound machine learning systems.

### Provable defense via convex outer adversarial polytope

The attack in this method is L infinite norm, while the paper mentioned other norms might also work, but they did not provide different norms’ experiments. It will bring a stronger back up if research could use various norm attack to validate the robustness. 
And a minor problem is scale of the classifier, but it have been fixed on another paper which we didn’t present today.


### TRADEs
Experiments on real datasets and adversarial competition demonstrate the effectiveness of the proposed algorithms. It would be interesting, however, to combine these methods with other related lines of research on adversarial defenses, e.g., feature denoising technique and network architecture design, to achieve more robust learning systems. 




